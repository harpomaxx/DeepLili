{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup PATH to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "PATH = 'sd_models_toyart/' #stable diffusion 1.5\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "from diffusers import UniPCMultistepScheduler\n",
    "vae = AutoencoderKL.from_pretrained(PATH , subfolder=\"vae\", local_files_only=True)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(PATH, subfolder=\"tokenizer\",local_files_only=True)\n",
    "text_encoder = CLIPTextModel.from_pretrained(PATH, subfolder=\"text_encoder\",local_files_only=True)\n",
    "unet = UNet2DConditionModel.from_pretrained(PATH, subfolder=\"unet\",local_files_only=True)\n",
    "scheduler = UniPCMultistepScheduler.from_pretrained(PATH, subfolder=\"scheduler\",local_files_only=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup cuda as default device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "torch_device = \"cuda\"\n",
    "vae.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "unet.to(torch_device)\n",
    "num_inference_steps = 2  # Number of denoising steps\n",
    "guidance_scale = 8.5  # Scale for classifier-free guidance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A function for generating multipl images\n",
    "I should be using a pipe (??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "def generate_images(prompt, tokenizer, text_encoder, unet, vae, scheduler, guidance_scale, n_samples, num_inference_steps):\n",
    "    pil_images = []\n",
    "    height = 512  # default height of Stable Diffusion\n",
    "    width = 512  # default width of Stable Diffusion\n",
    "    torch_device = \"cuda\"\n",
    "    seeds = [random.randint(1, 100) for _ in range(n_samples)]\n",
    "    \n",
    "    for seed in tqdm(seeds):\n",
    "        generator = torch.manual_seed(seed)\n",
    "        text_input = tokenizer(\n",
    "            prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "        max_length = text_input.input_ids.shape[-1]\n",
    "        uncond_input = tokenizer([\"\"], padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "        uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "        latents = torch.randn(\n",
    "            (1, unet.in_channels, height // 8, width // 8),\n",
    "            generator=generator,\n",
    "        )\n",
    "        latents = latents.to(torch_device)\n",
    "        latents = latents * scheduler.init_noise_sigma\n",
    "\n",
    "        scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        for t in scheduler.timesteps:\n",
    "            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "            latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "            # perform guidance\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "        latents = 1 / 0.18215 * latents\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image = vae.decode(latents).sample\n",
    "\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "        image = (image * 255).round().astype(\"uint8\")\n",
    "  \n",
    "        pil_image = Image.fromarray(image[0])\n",
    "        #display(pil_image)\n",
    "        pil_images.append(pil_image)\n",
    "    \n",
    "    return pil_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "prompt = [\"an black and bright white acrilic more white with five very small characters with teeths and mouth open in the sky and big tree in the sks style\"]\n",
    "#prompt = [\"a photo ultrarealistic the face of sks person [wearing ironman suit] from marvel movies, a detailed matte painting by Jeremy Geddes, cgsociety, space art, matte painting, redshift, concept art\"]\n",
    "\n",
    "#prompt = [\"a drawing of a very  old person practicing bodyboarding on grass lawn character design pixiv\"]\n",
    "#prompt = [\"a person bodyboarding on a grass GRASS  sled.fun,8k,insanely detailed,stunning environment, god rays\"]\n",
    "\n",
    "#prompt = [\"a man  with the face of sks riding on the back of a white horse through a forest, a detailed matte painting by Jeremy Geddes, cgsociety, space art, matte painting, redshift, concept art\"]\n",
    "#seeds = [441, 414, 41411, 4417, 156,121] # or any other list of seeds you want to use\n",
    "#seeds = [411414]\n",
    "n_samples =1\n",
    "images = generate_images(prompt, tokenizer, text_encoder, unet, vae, scheduler, guidance_scale, n_samples, num_inference_steps)\n",
    "display(images[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first attempt to use gradio    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install gradio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "# Define the Gradio wrapper function\n",
    "def gr_generate_images(prompt: str, num_images: int, num_inference: int):\n",
    "    prompt = prompt + \"sks style\"\n",
    "    images = generate_images(prompt, tokenizer, text_encoder, unet, vae, scheduler, guidance_scale, num_images, num_inference)\n",
    "    return images\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Column(variant=\"panel\"):\n",
    "        with gr.Row(variant=\"compact\"):\n",
    "            text = gr.Textbox(\n",
    "                label=\"Enter your prompt\",\n",
    "                show_label=False,\n",
    "                max_lines=1,\n",
    "                placeholder=\"a cute character\",\n",
    "            ).style(\n",
    "                container=False,\n",
    "            )\n",
    "            btn = gr.Button(\"Generate image\").style(full_width=False)\n",
    "          \n",
    "        with gr.Row(variant=\"compact\"):\n",
    "            num_images_slider = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=10,\n",
    "                step=1,\n",
    "                default=1,\n",
    "                label=\"Number of Images\",\n",
    "            )\n",
    "   \n",
    "            num_inference_steps_slider = gr.Slider(\n",
    "                minimum=1,\n",
    "                maximum=150,\n",
    "                step=1,\n",
    "                default=80,\n",
    "                label=\"Number of Inference Steps\",\n",
    "            )\n",
    "      \n",
    "        gallery = gr.Gallery(\n",
    "            label=\"Generated images\", show_label=False, elem_id=\"gallery\"\n",
    "        ).style(columns=[5], rows=[1], object_fit=\"contain\", height=\"200px\", width=\"200px\")\n",
    "\n",
    "    btn.click(gr_generate_images, [text, num_images_slider,num_inference_steps_slider], gallery)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
